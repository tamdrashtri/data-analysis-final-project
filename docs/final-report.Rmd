---
title: ""
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

# Introduction to Churn

Customer churn is one of the main issues in fields such as telecommunications, internet service providers, e-commerce, marketing, or banking. In the telecomunication industry, the market is saturated, making it very difficult to attract new customers. On the customer side, it is very easy to switch to a new provider if it provides a more beneficial offer (Canale and Lunardon, 2014). Because of this saturated market, the cost of aquiring new customers can be five to six times higher than retaining existing customers. Therefore, companies might be best invested in developing customers' trust on the service rather than attracting new customers. 

Researches on predicting customer churn have been done to try to predict whether a customer is likely to quit the service of their service provider and join a different one. Having an understanding of the influential factors causing customer churns on the service can help companies understand customers' needs and adjust on their provided services based on these factors to reduce churn. 

A lot of classification methods have been used to predict the rate of churn. One of those is logistic regression. 

The hypothesis for the following project is that given the same data set, all methods if accurated performed should yield similar results. All of these models will have the same question, which is, what predictive models perform the best among the models used in this project to predict the churn rate in the telecom industry. I hypothesize that complex models do not necessarily perform better than simplier models. 

# The description of the data set

Our data set consists of 7043 profiles of telecom customers and is available via.... The data contains of two main types of variables:

* Customers' personal characteristis such as their gender, whether they have a partner, their tenure status, whether they are senior citizens.
* There usage behaviors such as phone service, internet service, online security, tech support, streaming TV, streaming movies, contract, payment method and their month charges. 

Both of these types of variables are beneficial for our predictive analysis. It helps us to detect whether their decisions to churn is based on personal characteristics or on their service usage behaviors.

# Method

This project uses R to implement three predictive models: logistic regression, random forest and decision trees. I choose these models because they are often used in papers that have been published on churn. These are also well-known predictive models. 

Some of them will be implemented as black box, meaning I cannot know very well what are the processes they do the analysis on. Therefore, I will choose the dive deep into logistic regression and see if they are well performed. I choose logistic regression particularly because it is one of the models easiest to explain and measure performance. 
Bear in mind that these models will be briefly explained instead of looking into details. Specifically, I will mention its advantages and disadvantages of each of these models, then I will explain the steps I did to get the final results. 

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# EDA

We need some setup for our analysis. To do this, we need the simpleSetup package, which help us to install and load packages at the same time, in case we have not installed packages. This helps to foster reprocibility.

```{r setup, echo = TRUE, message=FALSE, warning=FALSE}
library(simpleSetup)

# SETUP-------

# this package helps to install and load packages at the same time.
library(simpleSetup)

# Load packages, install if needed
packages <- c('tidyverse', 'skimr', "GGally", "purrr", "repurrrsive",
              "nycflights13","gmodels", "stringr", "DataExplorer",
              "summarytools", "recipes", "broom", "modelr", "magrittr",
              "corrr", "viridis", "readr", "caret", "forcats", "tidyquant",
              "caret", "forcats", "tidyquant",
              # for decision tree
              "rpart", "rpart.plot", "ipred", "rsample")
library_install(packages)

churn <- read_csv("../data/WA_Fn-UseC_-Telco-Customer-Churn.csv")
```

For initial modelling process, it is helpful to split the dataset into two parts, one is for training the data, another one is for testing. I created the parition of 0.7 of the whole dataset, using the stratified sampling method.

We also drop `customerID` column because this unique ID does not give us any useful information.

```{r initial cleaning, echo=TRUE}
set.seed(123)

train.index <- createDataPartition(churn$Churn, p = .7, list = FALSE)
train <- churn[ train.index,]
test  <- churn[-train.index,]

test %<>%
  select(-customerID) %>%
  drop_na()

train %<>%
  select(-customerID) %>%
  drop_na()
```
 
# Feature Engineering

Several steps should be taken in order to transform the dataset into workable formats. 

1. First, we define Churn as our response variable.
2. We dummy code all columns having nominal values. 
3. We tranformed skewed distribution into lookable formats that look like normal distribution. 

To do this efficiently, we use `recipes` package, which helps us to prepare and bake those transformations quick and efficiently. The tranformed datasets are stored in bakedTrain and bakedTest.

```{r feature engineering, echo=TRUE}
recObj <- recipe(Churn ~ ., data = train) %>% # chose Churn as the response variable and the rest are explainatory variables
  step_dummy(all_nominal()) %>% # convert string to factor
  step_BoxCox(all_numeric()) %>%
  #step_discretize(all_numeric(), options = list(min_unique = 1)) %>%
  prep(data = train) # prepare for this data

bakedTrain <- bake(recObj, new_data = train) # use bake to transform according to recipes
bakedTest <- bake(recObj, new_data = test)
```

# Perform Logistic Regression

Logistic Regression helps us to predict binary categorical variable. This means that given all inputs of the function, the output would result in either 1 or 0.

We use `glm` function and specifies the family as `binomial` to indicates we want to use logistic regression for the following dataset. 

For our problem, since we need to predict churn in terms of multiple predictor variables, we will use multiple logistic regression for our analysis.

```{r}
# glm performs logistic regression to test the model

model1 <- glm(Churn_Yes ~ ., family = "binomial", data = bakedTrain)
summary(model1)
tidy(model1)

col_index <-
  data.frame(varImp(model1)) %>%
  mutate(names = factor(row.names(.))) %>%
  arrange(-Overall)

ggplot(col_index, aes(x = names, y = Overall)) +
  geom_segment(aes(xend = names, yend = 0)) +
  geom_point() +
  coord_flip() +
  scale_color_viridis() +
  theme_linedraw()

```

The following graph shows the importance of specific predictors influencing the outcome. The most important predictor is `Two year Contract`, following that are `One year contract`, and `Paperless Billing`.

Our following model is then used to predict the test dataset to see how much can the train dataset predict when it comes to new entries. 

What we might want to know is the error rate or the model. Our model error shows `0.19`. This error rate is not the best, but we also recognize that no model can have no errors.

A more interesting information we can gain is using confusion matrix

```{r}
#
test.predicted.m1 <- predict(model1, newdata = bakedTest, type = "response")

#table(bakedTest$Churn, test.predicted.m1 > 0.5) %>% prop.table() %>% round(3)

test %>%
  mutate(m1.pred = ifelse(test.predicted.m1 > 0.5, "Yes", "No")) %>%
  summarise(m1.error = mean(Churn != m1.pred))
  
```


The problem we might face from running multiple logistic regression is counfounding, in which there are correlations among predictors.
  
## Model Performance using ROC curve

After testing our result in the test dataset, we then turn into the ROC curve to evaluate the performance of the model. ROC is called the receiving operating characteristic curve, which is a visual measure 

The graphic shows the tradeoff between the rate at which the model correctly predict something and the rate of incorrectly predicting something. 

The result shows AUC score equals **0.85**. The AUC is the area under the ROC curve and ranges from 0.5 to 1. If the value is above 0.8, this indicates that the model is performing well to discriminate between categories comprising our response variable. 


```{r}
# Model Performance -------------------------------------------------------

library(ROCR)

prediction(test.predicted.m1, test$Churn) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

prediction(test.predicted.m1, test$Churn) %>%
  performance(measure = "auc") %>%
  .@y.values #0.85, 0.8414

```

# Decision Trees 

Decision trees divide the data set into smaller groups and fit a simple model to each group. This partitioning process uses successive binary partitions based on different explainatory variables. The result is based on the average response values of all observations in the smaller group. 

The model starts by looking at the entire data set and searches every instance of distinct value of every input variable to find a predictor and split value that partitions the data into two regions. The goal is to minimize the overall sums of squares error. 

Once we found the best splot, we split the data into two resulting regions and repeat the splitting process on each of the two regions. The process stops once it reaches its limit. However, while it can produce a very complex trees and makes very good predictions on the training data, it tend to overfit and lead to poor performance on unseen data.

```{r}
m1 <- rpart(
  formula = Churn_Yes ~ .,
  data    = bakedTrain,
  method  = "anova"
)

rpart.plot(m1)

plotcp(m1)


m2 <- rpart(
  formula = Churn_Yes ~ .,
  data    = bakedTrain,
  method  = "anova",
  control = list(cp = 0, xval = 10)
)

plotcp(m2)
abline(v = 12, lty = "dashed")

m1$cptable

m3 <- rpart(
  formula = Churn_Yes ~ .,
  data    = bakedTrain,
  method  = "anova",
  control = list(minsplit = 10, maxdepth = 12, xval = 10)
)

m3$cptable

hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)

head(hyper_grid)

nrow(hyper_grid)

models <- list()

for (i in 1:nrow(hyper_grid)) {

  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]

  # train a model and store in the list
  models[[i]] <- rpart(
    formula = Churn_Yes ~ .,
    data    = bakedTrain,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}

# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"]
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"]
}

hyper_grid %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
  ) %>%
  arrange(error) %>%
  top_n(-5, wt = error)

optimal_tree <- rpart(
  formula = Churn_Yes ~ .,
  data    = bakedTrain,
  method  = "anova",
  control = list(minsplit = 11, maxdepth = 8, cp = 0.01)
)

pred <- predict(optimal_tree, newdata = bakedTest)
RMSE(pred = pred, obs = bakedTest$Churn_Yes)

# make bootstrapping reproducible
set.seed(123)

# train bagged model
bagged_m1 <- bagging(
  formula = Churn_Yes ~ .,
  data    = bakedTrain,
  coob    = TRUE
)

bagged_m1

# assess 10-50 bagged trees
ntree <- 10:50

# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(123)

  # perform bagged model
  model <- bagging(
    formula = Churn_Yes ~ .,
    data    = bakedTrain,
    coob    = TRUE,
    nbagg   = ntree[i]
  )
  # get OOB error
  rmse[i] <- model$err
}

plot(ntree, rmse, type = 'l', lwd = 2)
abline(v = 25, col = "red", lty = "dashed")

# Specify 10-fold cross validation
ctrl <- trainControl(method = "cv",  number = 10)

# CV bagged model
bagged_cv <- train(
  Churn_Yes ~ .,
  data = bakedTrain,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
)

# assess results
bagged_cv


# plot most important variables
plot(varImp(bagged_cv), 20)

col_index <-
  data.frame(varImp(bagged_cv)) %>%
  mutate(names = factor(row.names(.))) %>%
  arrange(-Overall)

ggplot(col_index, aes(x = names, y = Overall)) +
  geom_segment(aes(xend = names, yend = 0)) +
  geom_point() +
  coord_flip() +
  scale_color_viridis() +
  theme_linedraw()

pred <- predict(bagged_cv, bakedTest)

RMSE(pred, bakedTest$Churn_Yes)
```

# Random Forest




```{r}
recObj <- recipe(Churn ~ ., data = train) %>% # chose Churn as the response variable and the rest are explainatory variables
  step_string2factor(Churn) %>%
  step_dummy(all_predictors()) %>% # convert string to factor
  step_BoxCox(all_numeric()) %>%
  #step_discretize(all_numeric(), options = list(min_unique = 1)) %>%
  prep(data = train) # prepare for this data

# recObj <- recipe(Churn ~ ., data = train) %>% # chose Churn as the response variable and the rest are explainatory variables
#   step_dummy(all_nominal()) %>% # convert string to factor
#   step_BoxCox(all_numeric()) %>%
#   #step_discretize(all_numeric(), options = list(min_unique = 1)) %>%
#   prep(data = train) # prepare for this data

bakedTrain <- bake(recObj, new_data = train) # use bake to transform according to recipes
bakedTest <- bake(recObj, new_data = test)



# for reproduciblity
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = Churn ~ .,
  data    = bakedTrain
)

m1

plot(m1)

varImpPlot(m1,
           sort = T,
           n.var=10,
           main="Top 10 - Variable Importance")

# Install and load required packages for decision trees and forests
library(rpart)
install.packages('randomForest')
library(randomForest)
install.packages('party')
library(party)

# Build Random Forest Ensemble
set.seed(415)
fit <- randomForest(Churn ~ .,
                    data= bakedTrain, importance=TRUE, ntree=2000)

fit
```


# Limitations and further study

In our following study, we face the following limitations:

